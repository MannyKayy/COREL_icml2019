{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from autoencoder import EncDec\n",
    "from dataset import ds_random_subset, ds_monkey_patch_target\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "from chofer_torchex.pershom import pershom_backend\n",
    "vr_l1_persistence = pershom_backend.__C.VRCompCuda__vr_persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_ROOT = '/scratch_nas/chofer/data'\n",
    "\n",
    "def dataset_factory(dataset=None, train_ratio=None):\n",
    "    \n",
    "    assert train_ratio is not None\n",
    "    assert dataset is not None\n",
    "    \n",
    "    cifar_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    tiny_imagenet_transform = transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    if dataset == 'cifar10':\n",
    "        ds = CIFAR10(\n",
    "            root=os.path.join(DS_ROOT, dataset),\n",
    "            train=True, \n",
    "            transform=cifar_transform, \n",
    "            download=False)   \n",
    "    elif dataset == 'cifar100':\n",
    "         ds = CIFAR100(\n",
    "            root=os.path.join(DS_ROOT, dataset),\n",
    "            train=True, \n",
    "            transform=cifar_transform, \n",
    "            download=False)   \n",
    "    elif dataset == 'tiny-imagenet-200':\n",
    "        ds = TinyImageNet(\n",
    "                root=os.path.join(DS_ROOT, dataset), \n",
    "                transform=tiny_imagenet_transform, \n",
    "                train=True)\n",
    "    else:\n",
    "        raise Exception()\n",
    "    \n",
    "    ds_monkey_patch_target(ds)\n",
    "    if train_ratio < 1.0:\n",
    "        ds = ds_random_subset(ds, train_ratio)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(x_hat, x, reduce=True):\n",
    "    l = (x - x_hat).abs().view(x.size(0), - 1).sum(dim=1)\n",
    "    if reduce:\n",
    "        l = l.mean() \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "def train(root_folder, config):\n",
    "    \n",
    "    train_args = config['train_args']\n",
    "    model_args = config['model_args']\n",
    "        \n",
    "    model = EncDec(**model_args).to(device)\n",
    "    \n",
    "    latent_dim = model.n_branches*model.out_features_branch\n",
    "    branch_siz = model.out_features_branch\n",
    "    ball_radius = 1.0 # hard-coded \n",
    "    \n",
    "    optim = Adam(\n",
    "        model.parameters(), \n",
    "        lr=train_args['learning_rate'])\n",
    "    \n",
    "    ds = dataset_factory(**config['data_args'])\n",
    "    dl = DataLoader(ds, \n",
    "                    batch_size=train_args['batch_size'],\n",
    "                    shuffle=True,\n",
    "                    drop_last=True)\n",
    "    \n",
    "    log = defaultdict(list)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(1,train_args['n_epochs']+1):\n",
    "            \n",
    "        for x,_ in dl:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            x_hat, z = model(x)\n",
    "            \n",
    "            top_loss = torch.tensor([0]).type_as(x_hat) \n",
    "            rec_loss = torch.tensor([0]).type_as(x_hat) \n",
    "            \n",
    "            rec_loss = l1_loss(x_hat, x, reduce=True)\n",
    "            \n",
    "            lifetimes = []\n",
    "            for i in range(0, latent_dim, branch_siz):\n",
    "                pers = vr_l1_persistence(\n",
    "                    z[:, i:i+branch_siz].contiguous(), # per-branch z_1,...,z_B\n",
    "                    0, 0, 'l1')[0][0] # [0][0] gives non-essential in H_0\n",
    "                \n",
    "                if pers.dim() == 2:\n",
    "                    pers = pers[:, 1] # all 0-dim. features have birth 0 in VR complex\n",
    "                    lifetimes.append(pers.tolist())\n",
    "                    top_loss += (pers - 2.0*ball_radius).abs().sum()\n",
    "            \n",
    "            log['lifetimes'].append(lifetimes)\n",
    "            log['top_loss'].append(top_loss.item())\n",
    "            log['rec_loss'].append(rec_loss.item())\n",
    "            \n",
    "            loss = train_args['rec_loss_w']*rec_loss + \\\n",
    "                   train_args['top_loss_w']*top_loss\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        print('{}: rec_loss: {:.4f} | top_loss: {:.4f}'.format(    \n",
    "            epoch, \n",
    "            np.array(log['rec_loss'][-int(len(ds)/train_args['batch_size']):]).mean()*train_args['rec_loss_w'],\n",
    "            np.array(log['top_loss'][-int(len(ds)/train_args['batch_size']):]).mean()*train_args['top_loss_w'])) \n",
    "        break\n",
    "\n",
    "    \n",
    "    basefile = os.path.join(root_folder, str(uuid.uuid4()))\n",
    "    \n",
    "    torch.save(model.state_dict(), '.'.join([basefile, 'model', 'pht']))\n",
    "    \n",
    "    out_data = [config, log]\n",
    "    file_ext = ['config', 'log']\n",
    "    for x,y in zip(out_data, file_ext):\n",
    "        with open('.'.join([basefile, y, 'pickle']), 'wb') as fid:\n",
    "            pickle.dump(x, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_args'  : {\n",
    "        'dataset'     : 'cifar100',\n",
    "        'train_ratio' : 0.5\n",
    "    },\n",
    "    'model_args' : {},\n",
    "    'train_args' : {\n",
    "        'n_epochs'      : 50,\n",
    "        'learning_rate' : 1e-3,\n",
    "        'batch_size'    : 100,\n",
    "        'top_loss_w'    : 10.0,\n",
    "        'rec_loss_w'    : 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "#train('/tmp', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/tmp/config.json', 'w') as fid:\n",
    "    json.dump(config, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_args': {'dataset': 'cifar100', 'train_ratio': 0.5}, 'model_args': {}, 'train_args': {'n_epochs': 50, 'learning_rate': 0.001, 'batch_size': 100, 'top_loss_w': 10.0, 'rec_loss_w': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "with open('/tmp/config.json', 'r') as fid:\n",
    "    check = json.load(fid)\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
